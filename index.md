# Pichao Wang, PhD, Amazon AGI Foundations

<img style="float: right; padding-left:15px; width:150px" src="./images/WangPiChao.jpg" >

[**AI 2000 Most Influential Scholar**](https://www.aminer.cn/ai2000/search_rank?id=562c81d345cedb3398c44362)

[**World's Top 1% Scientist named by Stanford University**](https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/6)

[**CVPR 2022 Best Student Paper Recipient**](https://cvpr2022.thecvf.com/cvpr-2022-paper-awards)


Email: pichaowang@gmail.com   [Goolge Scholar](https://scholar.google.com/citations?user=QozdnnoAAAAJ&hl=en) [ResearchGate](https://www.researchgate.net/profile/Pichao-Wang) [Linkedin](https://www.linkedin.com/in/pichao-wang-494773109/)


## Recent News

1. 2024-09: One paper about text-to-video generation was accepted by **NeurIPS 2024**
2. 2024-09: One paper about diffusion-based text-to-video retrieval was accepted by **NeurIPS 2024**
3. 2024-09: One paper about MLLM for video segmentation was accepted by **NeurIPS 2024**
4. 2024-07: One paper about interpretable image recognition was accepted by **ACM MM 2024**
5. 2024-07: One paper about camouflaged instance segmentation was accepted by **ACM MM 2024**
6. 2024-02: One paper about text-to-video retreival was accepted by **CVPR2024**
7. 2024-02: One paper about 3D pose estimation was accepted by **CVPR2024**
8. 2024-01: One invited paper was accepted by **TPAMI**
9. 2024-01: One paper about action recognition was accepted by **ESWA**
10. 2023-09: One paper about large model finetuning was accepted by **IJCV**
11. 2023-07: One paper about RGB-D action recognition was accepted by **ACM MM 2023**
12. 2023-07: One paper about vision transformer was accepted by **ICCV 2023**
13. 2023-07: One paper about text-to-video retrieval was accepted by **ICCV 2023**
14. 2023-05: One paper about vision transformer was accepted by **IJCV**
15. 2023-04: One paper about RGB+D action recognition was accepted by **TPAMI**
16. 2023-04: One paper about 3D pose estimation was accepted by **Pattern Recognition**
17. 2023-02: One paper about efficient vision transformer was accepted by **CVPR2023**
18. 2023-02: One paper about long form video understanding was accepted by **CVPR2023**
19. 2023-02: One paper about 3D pose estimation was accepted by **CVPR2023**
20. 2022-12: Our TIP'21 paper received the IEEE Finland SP/CAS Best Paper Award
21. 2022-11: One paper about Semantic Segmentation was accepted by **AAAI2023**
22. 2022-11: One paper about Neural Style Transfer was accepted by **AAAI2023**
23. 2022-09: One paper about skeleton action recognition was accepted by **ACCV 2022**
24. 2022-09: One paper about vision transformer compression was accepted by **NeurIPS 2022**
25. 2022-07: One paper about vision transformer was accepted by **ECCV 2022**
26. 2022-07: One paper about unsupervised semantic segmentation was accepted by **ECCV 2022**
27. 2022-06: **Best Student Paper Award** in **CVPR 2022**.
28. 2022-03: One paper about 3D human pose estimation was accepted by **CVPR 2022**.
29. 2022-03: One paper about 3D object detection was accepted by **CVPR 2022**.
30. 2022-03: One paper about RGB+D motion recognition was accepted by **CVPR 2022**.
31. 2022-01: One paper about knowledge distillation was accepted by **ICASSP 2022**.
32. 2022-01: One paper about unsupervised domain adaption was accepted by **ICLR 2022**.
33. 2021-12: One paper about pose estimation was accepted by **IEEE TMM**. 
34. 2021-12: One paper about vision transformer training was accepted by **AAAI 2022**.
35. 2021-07: One paper about Object ReID was accepted by **ICCV 2021**.
36. 2021-07: One paper about Zero-Shot NAS was accepted by **ICCV 2021**.
37. 2021-06: One paper about video object detection was accepted by **IJCV**.
38. 2021-06: One paper about video object detection was accepted by **IEEE TCSVT**.


## Biography

I am a senior research scientist at Amazon AGI Foundations. Before I joined Amazon, I worked as a staff/senior engineer at DAMO Academy, Alibaba Group (U.S.) for more than 4 years. I received my Ph.D in Computer Science from [University of Wollongong](https://www.uow.edu.au/media/2022/new-high-for-uow-in-qs-world-university-rankings.php), Australia, in Oct. 2017, supervised by Prof. [Wanqing Li](https://sites.google.com/view/wanqingli/home-news) and Prof. [Philip Ogunbona](https://documents.uow.edu.au/~philipo/).  I received  my M.E. in Information and Communication Engineering from [Tianjin University](http://www.tju.edu.cn/english/index.htm), China, in 2013, supervised by Prof. [Yonghong Hou](http://seea.tju.edu.cn/info/1122/2098.htm), and B.E. in Network Engineering from [Nanchang University](http://english.ncu.edu.cn/), China, in 2010.

## Research Interests

Computer Vision · Multimedia · Deep Learning · Image Representation · Video Understanding

## Selected Awards and Honors

1. Apr. 2024, The Tony Stark Award of Prime Video

2. Oct. 2023, World’s Top 1% Scientist

3. Jun.2022, [Best Student Paper Award](https://cvpr2022.thecvf.com/cvpr-2022-paper-awards) @CVPR2022

4. Jan.2022, AI 2000 Most Influential Scholars [certificate](https://github.com/wangpichao/wangpichao.github.io/blob/main/images/miner.png)

5. Oct.2021, World’s Top 2% Scientists

6. Jun. 2020, Second Prize, Multiple Object Tracking and Segmentation@CVPR2020 

7. May. 2018, EIS Faculty Postgraduate Thesis Award.

8. Aug. 2017, Second Prize, Action, Gesture, and Emotion Recognition Workshop and Competitions: Large Scale Multimodal Gesture Recognition and Real versus Fake expressed emotions@ICCV2017

9. Apr. 2017 First Prize (Winner), Large Scale 3D Human Activity Analysis Challenge in Depth Video@ICME2017

10. Dec. 2016 Second Prize, Joint Contest on Multimedia Challenges Beyond Visual Analysis@ICPR2016

11. Dec. 2016 Third Prize, Joint Contest on Multimedia Challenges Beyond Visual Analysis@ICPR2016

12. Jan. 2013 Excellent Postgraduate Award

13. Dec. 2011 Excellent Prize, National Campus CUDA Programming Contest. [certificate](https://sites.google.com/site/pichaossites/resources/img051.jpg?attredirects=0&d=1)


## Publications

### Ph.D. Dissertation

<div style="display: flex; flex-direction: row; align-items: stretch; padding: 15px; border-bottom: 1px solid #ccc; margin-bottom: 20px;">
    <div style="flex-shrink: 0; margin-right: 15px;">
        <img src="./images/architecture.png" style="width: 150px; height: 100%; object-fit: cover; border-radius: 8px;" alt="Thesis Thumbnail">
    </div>
    <div style="flex-grow: 1;">
        <p style="font-size: 16px; font-weight: bold;">Action Recognition from RGB-D Data. The University of Wollongong, 2017. (Best Postgraduate Thesis Award)</p>
        <p><a href="http://ro.uow.edu.au/theses1/112/" target="_blank">Read Thesis</a></p>
    </div>
</div>

### Preprint (selected papers, [full paper list](https://scholar.google.com/citations?user=QozdnnoAAAAJ&hl=en))

<div style="display: flex; flex-direction: row; align-items: stretch; padding: 15px; border-bottom: 1px solid #ccc; margin-bottom: 20px;">
    <div style="flex-shrink: 0; margin-right: 15px;">
        <img src="./images/architecture.png" style="width: 150px; height: 100%; object-fit: cover; border-radius: 8px;" alt="Paper Thumbnail">
    </div>
    <div style="flex-grow: 1;">
        <p style="font-size: 16px; font-weight: bold;">Self-Supervised Pre-Training for Transformer-Based Person Re-Identification</p>
        <p>Hao Luo, <strong>Pichao Wang</strong>, Yi Xu, Feng Ding, Yanxin Zhou, Fan Wang, Hao Li, Rong Jin. arXiv 2021.</p>
        <p><a href="https://arxiv.org/pdf/2111.12084.pdf" target="_blank">Read Paper</a> | <a href="https://github.com/damo-cv/TransReID-SSL" target="_blank">Code</a></p>
    </div>
</div>

### Conference Papers (selected papers, [full paper list](https://scholar.google.com/citations?user=QozdnnoAAAAJ&hl=en))

<div style="display: flex; flex-direction: row; align-items: stretch; padding: 15px; border-bottom: 1px solid #ccc; margin-bottom: 20px;">
    <div style="flex-shrink: 0; margin-right: 15px;">
        <img src="./images/architecture.png" style="width: 150px; height: 100%; object-fit: cover; border-radius: 8px;" alt="Paper Thumbnail">
    </div>
    <div style="flex-grow: 1;">
        <p style="font-size: 16px; font-weight: bold;">1.Enhancing Motion in Text-to-Video Generation with Decomposed Encoding and Conditioning</p>
        <p>Penghui Ruan, <strong>Pichao Wang</strong>, Divya Saxena, Jiannong Cao, Yuhui Shi.</p>
        <p>NeurIPS 2024.</p>
    </div>
</div>

<div style="display: flex; flex-direction: row; align-items: stretch; padding: 15px; border-bottom: 1px solid #ccc; margin-bottom: 20px;">
    <div style="flex-shrink: 0; margin-right: 15px;">
        <img src="./images/architecture.png" style="width: 150px; height: 100%; object-fit: cover; border-radius: 8px;" alt="Paper Thumbnail">
    </div>
    <div style="flex-grow: 1;">
        <p style="font-size: 16px; font-weight: bold;">2.Diffusion-Inspired Truncated Sampler for Text-Video Retrieval</p>
        <p>Jiamian Wang, <strong>Pichao Wang</strong>, Dongfang Liu, Qiang Guan, Sohail Dianat, Majid Rabbani, Raghuveer Rao, Zhiqiang Tao. </p>
        <p>NeurIPS 2024.</p>
    </div>
</div>

<div style="display: flex; flex-direction: row; align-items: stretch; padding: 15px; border-bottom: 1px solid #ccc; margin-bottom: 20px;">
    <div style="flex-shrink: 0; margin-right: 15px;">
        <img src="./images/architecture.png" style="width: 150px; height: 100%; object-fit: cover; border-radius: 8px;" alt="Paper Thumbnail">
    </div>
    <div style="flex-grow: 1;">
        <p style="font-size: 16px; font-weight: bold;">3.One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos</p>
        <p>Zechen Bai, Tong He, Haiyang Mei, <strong>Pichao Wang</strong>, Ziteng Gao, Joya Chen, Lei Liu, Zheng Zhang, Mike Zheng Shou.</p>
        <p>NeurIPS 2024.</p>
    </div>
</div>

<div style="display: flex; flex-direction: row; align-items: stretch; padding: 15px; border-bottom: 1px solid #ccc; margin-bottom: 20px;">
    <div style="flex-shrink: 0; margin-right: 15px;">
        <img src="./images/architecture.png" style="width: 150px; height: 100%; object-fit: cover; border-radius: 8px;" alt="Paper Thumbnail">
    </div>
    <div style="flex-grow: 1;">
        <p style="font-size: 16px; font-weight: bold;">4.Align2Concept: Language Guided Interpretable Image Recognition by Visual Prototype and Textural Concept Alignment</p>
        <p>Jiaqi Wang, <strong>Pichao Wang</strong>, Yi Feng, Huafeng Liu, Chang Gao, Liping Jing.</p>
        <p>ACM MM 2024.</p>
    </div>
</div>

<div style="display: flex; flex-direction: row; align-items: stretch; padding: 15px; border-bottom: 1px solid #ccc; margin-bottom: 20px;">
    <div style="flex-shrink: 0; margin-right: 15px;">
        <img src="./images/architecture.png" style="width: 150px; height: 100%; object-fit: cover; border-radius: 8px;" alt="Paper Thumbnail">
    </div>
    <div style="flex-grow: 1;">
        <p style="font-size: 16px; font-weight: bold;">5.Adaptive Query Selection for Camouflaged Instance Segmentation</p>
        <p>Bo Dong, <strong>Pichao Wang</strong>, Hao Luo, Fan Wang.</p>
        <p>ACM MM 2024.</p>
    </div>
</div>

<div style="display: flex; flex-direction: row; align-items: stretch; padding: 15px; border-bottom: 1px solid #ccc; margin-bottom: 20px;">
    <div style="flex-shrink: 0; margin-right: 15px;">
        <img src="./images/architecture.png" style="width: 150px; height: 100%; object-fit: cover; border-radius: 8px;" alt="Paper Thumbnail">
    </div>
    <div style="flex-grow: 1;">
        <p style="font-size: 16px; font-weight: bold;">6.Text is MASS: Modelling as Stochastic Embedding for Text-to-Video Retrieval</p>
        <p>Jiamian Wang, Guohao Sun, <strong>Pichao Wang</strong>, Dongfang Liu, Sohail Dianat, Majid Rabbani, Raghuveer Rao, Zhiqiang Tao. </p>
        <p>CVPR 2024 (Highlight).</p>
        <p><a href="https://arxiv.org/pdf/2403.17998.pdf" target="_blank">Read Paper</a> | <a href="https://github.com/Jiamian-Wang/T-MASS-text-video-retrieval" target="_blank">Code</a></p>
    </div>
</div>

<div style="display: flex; flex-direction: row; align-items: stretch; padding: 15px; border-bottom: 1px solid #ccc; margin-bottom: 20px;">
    <div style="flex-shrink: 0; margin-right: 15px;">
        <img src="./images/architecture.png" style="width: 150px; height: 100%; object-fit: cover; border-radius: 8px;" alt="Paper Thumbnail">
    </div>
    <div style="flex-grow: 1;">
        <p style="font-size: 16px; font-weight: bold;">7.Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose Estimation</p>
        <p>Wenhao Li, Mengyuan Liu, Hong Liu, <strong>Pichao Wang</strong>, Jialun Cai, and Nicu Sebe.</p>
        <p>CVPR 2024 (Highlight).</p>
    </div>
</div>

<div style="display: flex; flex-direction: row; align-items: stretch; padding: 15px; border-bottom: 1px solid #ccc; margin-bottom: 20px;">
    <div style="flex-shrink: 0; margin-right: 15px;">
        <img src="./images/architecture.png" style="width: 150px; height: 100%; object-fit: cover; border-radius: 8px;" alt="Paper Thumbnail">
    </div>
    <div style="flex-grow: 1;">
        <p style="font-size: 16px; font-weight: bold;">8.Multi-stage Factorized Spatio-Temporal Representation for RGB-D Action and Gesture Recognition</p>
        <p>Yujun Ma, Benjia Zhou, Ruili Wang, <strong>Pichao Wang</strong>. </p>
        <p>ACM MM 2023.</p>
    </div>
</div>

<div style="display: flex; flex-direction: row; align-items: stretch; padding: 15px; border-bottom: 1px solid #ccc; margin-bottom: 20px;">
    <div style="flex-shrink: 0; margin-right: 15px;">
        <img src="./images/architecture.png" style="width: 150px; height: 100%; object-fit: cover; border-radius: 8px;" alt="Paper Thumbnail">
    </div>
    <div style="flex-grow: 1;">
        <p style="font-size: 16px; font-weight: bold;">9.Revisiting Vision Transformer from the View of Path Ensemble</p>
        <p>Shuning Chang, <strong>Pichao Wang</strong>, Hao Luo, Fan Wang, Mike Zheng Shou. </p>
        <p></p><strong>Oral</strong>, ICCV 2023.</p>
    </div>
</div>

<div style="display: flex; flex-direction: row; align-items: stretch; padding: 15px; border-bottom: 1px solid #ccc; margin-bottom: 20px;">
    <div style="flex-shrink: 0; margin-right: 15px;">
        <img src="./images/architecture.png" style="width: 150px; height: 100%; object-fit: cover; border-radius: 8px;" alt="Paper Thumbnail">
    </div>
    <div style="flex-grow: 1;">
        <p style="font-size: 16px; font-weight: bold;">10.Audio-Enhanced Text-to-Video Retrieval using Text-Conditioned Feature Alignment</p>
        <p>Sarah Ibrahimi, Xiaohang Sun, <strong>Pichao Wang</strong>, Amanmeet Garg, Ashutosh Sanan, Mohamed Omar.</p>
        <p></p><strong>Oral</strong>, ICCV 2023.</p>
    </div>
</div>

<div style="display: flex; flex-direction: row; align-items: stretch; padding: 15px; border-bottom: 1px solid #ccc; margin-bottom: 20px;">
    <div style="flex-shrink: 0; margin-right: 15px;">
        <img src="./images/architecture.png" style="width: 150px; height: 100%; object-fit: cover; border-radius: 8px;" alt="Paper Thumbnail">
    </div>
    <div style="flex-grow: 1;">
        <p style="font-size: 16px; font-weight: bold;">11.Making Vision Transformers Efficient from A Token Sparsification View</p>
        <p>Shuning Chang, <strong>Pichao Wang</strong>, Ming Lin, Fan Wang, David Junhao Zhang, Rong Jin. </p>
        <p>CVPR 2023.</p>
        <p><a href="https://arxiv.org/pdf/2303.08685.pdf" target="_blank">Read Paper</a> | <a href="https://github.com/changsn/STViT-R" target="_blank">Code</a></p>
    </div>
</div>

<div style="display: flex; flex-direction: row; align-items: stretch; padding: 15px; border-bottom: 1px solid #ccc; margin-bottom: 20px;">
    <div style="flex-shrink: 0; margin-right: 15px;">
        <img src="./images/architecture.png" style="width: 150px; height: 100%; object-fit: cover; border-radius: 8px;" alt="Paper Thumbnail">
    </div>
    <div style="flex-grow: 1;">
        <p style="font-size: 16px; font-weight: bold;">12.PoseFormerV2: Exploring Frequency Domain for Efficient and Robust 3D Human Pose Estimation</p>
        <p>Qitao Zhao, Ce Zheng, Mengyuan Liu, <strong>Pichao Wang</strong>, Chen Chen.</p>
        <p>CVPR 2023.</p>
        <p><a href="https://arxiv.org/pdf/2303.17472.pdf" target="_blank">Read Paper</a> | <a href="https://github.com/QitaoZhao/PoseFormerV2" target="_blank">Code</a></p>
    </div>
</div>

<div style="display: flex; flex-direction: row; align-items: stretch; padding: 15px; border-bottom: 1px solid #ccc; margin-bottom: 20px;">
    <div style="flex-shrink: 0; margin-right: 15px;">
        <img src="./images/architecture.png" style="width: 150px; height: 100%; object-fit: cover; border-radius: 8px;" alt="Paper Thumbnail">
    </div>
    <div style="flex-grow: 1;">
        <p style="font-size: 16px; font-weight: bold;">13.Selective Structured State-Spaces for Long-Form Video Understanding</p>
        <p>Jue Wang, Wentao Zhu, <strong>Pichao Wang</strong>, Xiang Yu, Linda Liu, Mohamed Omar, Raffay Hamid.</p>
        <p>CVPR 2023.</p>
        <p><a href="https://arxiv.org/pdf/2303.14526.pdf" target="_blank">Read Paper</a></p>
    </div>
</div>

<div style="display: flex; flex-direction: row; align-items: stretch; padding: 15px; border-bottom: 1px solid #ccc; margin-bottom: 20px;">
    <div style="flex-shrink: 0; margin-right: 15px;">
        <img src="./images/architecture.png" style="width: 150px; height: 100%; object-fit: cover; border-radius: 8px;" alt="Paper Thumbnail">
    </div>
    <div style="flex-grow: 1;">
        <p style="font-size: 16px; font-weight: bold;">14.Head-Free Lightweight Semantic Segmentation with Linear Transformer</p>
        <p>Bo Dong, <strong>Pichao Wang</strong>, Fan Wang. </p>
        <p>AAAI 2023.</p>
        <p><a href="https://arxiv.org/pdf/2301.04648.pdf" target="_blank">Read Paper</a> | <a href="https://github.com/dongbo811/AFFormer" target="_blank">Code</a></p>
    </div>
</div>



15. Dongyang Li, Hao Luo, **Pichao Wang**, Zhibin Wang, Shang Liu, Fan Wang, "Frequency Domain Disentanglement for Arbitrary Neural Style Transfer", AAAI 2023.

16. Zhenyu Wang, Hao Luo, **Pichao Wang**, Feng Ding, Fan Wang, Hao Li, "VTC-LFC: Vision Transformer Compression with Low-Frequency Components", NeurIPS 2022.[paper](https://openreview.net/pdf?id=HuiLIB6EaOk) [code](https://github.com/Daner-Wang/VTC-LFC)

17. **Pichao Wang**, *Xue Wang*, Fan Wang, Ming Lin, Shuning Chang, Hao Li, Rong Jin, (first two authors make equal contributions), "KVT: k-NN Attention for Boosting Vision Transformers", ECCV 2022. [paper](https://arxiv.org/pdf/2106.00515.pdf). [code](https://github.com/damo-cv/KVT)

18. Zhaoyuan Yin, **Pichao Wang**@, Fan Wang, Xianzhe Xu, Hanling Zhang, Hao Li, Rong Jin,(@ Corresponding author), "TransFGU: A Top-down Approach to Fine-Grained Unsupervised Semantic Segmentation", ECCV 2022, ***Oral***(2.7% of submitted papers) [paper](https://arxiv.org/pdf/2112.01515.pdf). [code](https://github.com/damo-cv/TransFGU)

19. Benjia Zhou, **Pichao Wang**@, Jun Wan, Yanyan Liang, Fan Wang, Du Zhang, Zhen Lei, Hao Li, Rong Jin, (@ Corresponding author), "Decoupling and Recoupling Spatiotemporal Representation for RGB-D-based Motion Recognition", CVPR 2022. [paper](https://arxiv.org/pdf/2112.09129.pdf). [code](https://github.com/damo-cv/MotionRGBD)

20. Hansheng Chen, **Pichao Wang**@, Fan Wang, Wei Tian, Lu Xiong, Hao Li, (@ Corresponding author), "EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation", CVPR 2022, ***Best Student Paper Award***. [paper](https://arxiv.org/pdf/2203.13254.pdf) [code](https://github.com/tjiiv-cprg/EPro-PnP)

21. Wenhao Li, Hong Liu, Hao Tang, **Pichao Wang**, Luc Van Gool, "MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation", CVPR 2022. [paper](https://arxiv.org/pdf/2111.12707.pdf). [code](https://github.com/Vegetebird/MHFormer)

22. **Pichao Wang**, Fan Wang, Hao Li, "Image-to-Video Re-Identification via Mutual Discriminative Knowledge Transfer", ICASSP 2022. [paper](https://www.researchgate.net/publication/358008989_IMAGE-TO-VIDEO_RE-IDENTIFICATION_VIA_MUTUAL_DISCRIMINATIVE_KNOWLEDGE_TRANSFER)

23. Tongkun Xu, Weihua Chen, **Pichao Wang**, Fan Wang, Hao Li, Rong Jin, "Cdtrans: Cross-domain transformer for unsupervised domain adaptation", ICLR 2022. [paper](https://arxiv.org/pdf/2109.06165.pdf). [code](https://github.com/CDTrans/CDTrans)

24. **Pichao Wang**, *Xue Wang*, Hao Luo, Jingkai Zhou, Zhipeng Zhou, Fan Wang, Hao Li, and Rong Jin,(first two authors make equal contributions), "Scaled relu matters for training vision transformers", AAAI 2022. [paper](https://arxiv.org/abs/2109.03810). [video](https://recorder-v3.slideslive.com/#/share?share=57843&s=123e71d8-018a-41fa-a13f-1f18a969188c)

25. Shuting He, Hao Luo, **Pichao Wang**, Fan Wang, Hao Li, and Wei Jiang, "TransReid: Transformer-based Object Re-identification",ICCV 2021. [paper](https://openaccess.thecvf.com/content/ICCV2021/papers/He_TransReID_Transformer-Based_Object_Re-Identification_ICCV_2021_paper.pdf). [code](https://github.com/damo-cv/TransReID)

26. Min Lin, **Pichao Wang**, Zhenhong Sun, Hesen Chen, Xiuyu Sun, Qi Qian, Hao Li, and Rong Jin, "Zen-NAS: A Zero-Shot NAS for High-Performance Deep Image Recognition", ICCV 2021. [paper](https://arxiv.org/pdf/2102.01063.pdf). [code](https://github.com/idstcv/ZenNAS)
 
27. *Liang Han*, **Pichao Wang**, Zhaozheng Yin, Fan Wang, and Hao Li, (*first two authors make equal contributions*) "Exploiting Better Feature Aggregation for Video Object Detection", ACM MM 2020. [paper](https://par.nsf.gov/servlets/purl/10289758)

28. Chang Tang, Xinwang Liu, Xinzhong Zhu, En Zhu, Kun Sun, **Pichao Wang**, Lizhe Wang and Albert Zomaya, "R2MRF: Defocus Blur Detection via Recurrently Refining Multi-scale Residual Features", AAAI 2020.[paper](https://ojs.aaai.org/index.php/AAAI/article/download/6884/6738). [code](https://github.com/ChangTang/R2MRF)

29. **Pichao Wang**, Wanqing Li, Jun Wan, Philip Ogunbona, and Xinwang Liu, "Cooperative Training of Deep Aggregation Networks for RGB-D Action Recognition", AAAI 2018, ***ORAL*** [paper](https://sites.google.com/site/pichaossites/resources/AAAI2018_action.pdf?attredirects=0&d=1). [code](https://sites.google.com/site/pichaossites/resources/codesAAAI.zip?attredirects=0&d=1)
  
30. *Huogen Wang*, **Pichao Wang**, Zhanjie Song, and Wanqing Li, (first two authors make equal contributions) "Large-scale Multimodal Gesture Recognition Using Heterogeneous Networks", ICCV 2017.[paper](https://sites.google.com/site/pichaossites/resources/Heterogeneous.pdf?attredirects=0&d=1). [code](https://github.com/wanghuogen/ConGestureChallenge4ICCV)
  
31. *Huogen Wang*, **Pichao Wang**, Zhanjie Song, and Wanqing Li, (first two authors make equal contributions) "Large-scale Multimodal Gesture Segmentation and Recognition based on Convolutional Neural Network", ICCV 2017. [paper](https://sites.google.com/site/pichaossites/resources/CNN_ICCV_2017_paper.pdf?attredirects=0&d=1). [code](https://github.com/wanghuogen/ConGestureChallenge4ICCV-DF)
  
32. **Pichao Wang**, Wanqing Li, Zhimin Gao, Yuyao Zhang, Chang Tang, and Philip Ogunbona, "Scene Flow to Action Map: A New Representation for RGB-D Based Action Recognition with Convolutional Neural Networks", CVPR 2017.  [paper](https://arxiv.org/pdf/1702.08652.pdf)
   
33. **Pichao Wang**, *Zhaoyang Li*, Yonghong Hou, and Wanqing Li, (first two authors make equal contributions) "Action Recognition Based on Joint Trajectory Maps Using Convolutional Neural Networks", ACM MM 16. [paper](https://sites.google.com/site/pichaossites/resources/p102-wang.pdf?attredirects=0&d=1). [code](https://sites.google.com/site/pichaossites/resources/JTM-rotate-ntu.zip?attredirects=0&d=1)
  
34. **Pichao Wang**, Wanqing Li, Zhimin Gao, Chang Tang, Jing Zhang, and Philip Ogunbona,"ConvNets-Based Action Recognition from Depth Maps Through Virtual Cameras and Pseudocoloring", ACM MM 15. [paper](https://sites.google.com/site/pichaossites/resources/ConvNet%20Action%20Recognition.pdf?attredirects=0&d=1). [code](https://sites.google.com/site/pichaossites/resources/THMS.zip?attredirects=0&d=1)

### Journal Articles (selected papers, [full paper list](https://scholar.google.com/citations?user=QozdnnoAAAAJ&hl=en))

1. Hansheng Chen, Wei Tian, **Pichao Wang**, Fan Wang, Lu Xiong, Hao Li, "EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation", IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2024, [paper](https://arxiv.org/pdf/2203.13254.pdf) [code](https://github.com/tjiiv-cprg/EPro-PnP)
   
2. Henry Hengyuan Zhao,  **Pichao Wang**, Yuyang Zhao, Hao Luo, Fan Wang, Mike Zheng Shou, "SCT: A Simple Baseline for Parameter-Efficient Fine-Tuning via Salient Channels", International Journal of Computer Vision (IJCV), 2023. [paper](https://arxiv.org/pdf/2309.08513v1.pdf). [code](https://github.com/zhaohengyuan1/SCT)
   
3. Jingkai Zhou, **Pichao Wang**@, Jiasheng Tang, Fan Wang, Qiong Liu, Hao Li, Rong Jin,(@project lead), "What limits the performance of local self-attention?", International Journal of Computer Vision (IJCV), 2023. [paper](https://arxiv.org/pdf/2112.12786.pdf) [code](https://github.com/damo-cv/ELSA)

4. Benjia Zhou, **Pichao Wang**@, Jun Wan, Liangliang Yan, and Fan Wang, (@corresponding auther), "A Unified Multimodal De-and Re-coupling Framework for RGB-D Motion Recognition", IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2023. [paper](https://arxiv.org/pdf/2211.09146.pdf) [code](https://github.com/zhoubenjia/MotionRGBD-PAMI)

5. Wenhao Li, Hong Liu, Hao Tang, and **Pichao Wang**, "Multi-Hypothesis Representation Learning for Transformer-Based 3D Human Pose Estimation", Pattern Recognition, 2023

6. Wenhao Li, Hong Liu, Runwei Ding, Mengyuan Liu, **Pichao Wang**, and Wenming Yang, "Exploiting Temporal Contexts with Strided Transformer for 3D Human Pose Estimation", IEEE Transactions on Multimedia, 2021. [paper](https://arxiv.org/pdf/2103.14304.pdf). [code](https://github.com/Vegetebird/StridedTransformer-Pose3D)

7.  *Liang Han*, **Pichao Wang**, Zhaozheng Yin, Fan Wang, and Hao Li, (first two authors make equal contributions), "Context and Structure Mining Network for Video Object Detection", International Journal of Computer Vision (IJCV), 2021. [paper](https://link.springer.com/article/10.1007/s11263-021-01507-2)

8. *Liang Han*, **Pichao Wang**, Zhaozheng Yin, Fan Wang, and Hao Li, (first two authors make equal contributions), "Class-aware Feature Aggregation Network for Video Object Detection",
IEEE Transactions on Circuits and Systems for Video Technology, 2021. [paper](https://ieeexplore.ieee.org/abstract/document/9474502)

9. Zitong Yu, Benjia Zhou, Jun Wan, **Pichao Wang**, Haoyu Chen, Xin Liu, Stan Z Li, and Guoying Zhao, "Searching Multi-Rate and Multi-Modal Temporal Enhanced Network for Gesture Recognition", IEEE Transaction on Image Processing, 2021. [paper](https://ieeexplore.ieee.org/abstract/document/9454270). [code](https://github.com/ZitongYu/3DCDC-NAS)

10. Xiangyu Li, Yonghong Hou, **Pichao Wang**@, Zhimin Gao, Mingliang Xu, and Wanqing Li,（@ Corresponding author), "Trear: Tranformer-based RGB-D Egocentric Action Recognition", IEEE Transactions on Cognitive and Developmental System, 2021. [paper](https://ieeexplore.ieee.org/abstract/document/9312201)

11. Chang Tang, Xinwang Liu, Shan An, and **Pichao Wang**, "BR2NET: Defocus Blur Detection via Bidirectional Channel Attention Residual Refining Network", IEEE Transactions on Multimedia, 2020. [paper](https://ieeexplore.ieee.org/abstract/document/9057632)

12. Chang Tang, Xinwang Liu, **Pichao Wang**, Changqing Zhang, Miaomiao Li and Lizhe Wang,“Adaptive Hypergraph Embedded Semi-supervised Multi-label Image Annotation”
 IEEE Transactions on Multimedia, 2019. [paper](https://ieeexplore.ieee.org/abstract/document/8684404)
 
13. Chang Tang, Xinzhong Zhu, Xinwang Liu, Miaomiao Li, **Pichao Wang**, Changqing Zhang and Lizhe Wang, “Learning Joint Affinity Graph for Multi-view Subspace Clustering”, IEEE Transactions on Multimedia, 2019. [paper](https://ieeexplore.ieee.org/abstract/document/8587193)

14. Chuankun Li, Yonghong Hou, **Pichao Wang**@, and Wanqing Li, (@Corresponding author), "Multi-view Based 3D Action Recognition Using Deep Networks",
IEEE Transactions on Human Machine Systems, 2018. [paper](https://ieeexplore.ieee.org/abstract/document/8584131)

15. Chang Tang, Wanqing Li, **Pichao Wang**@, and Lizhe Wang, (@ Corresponding author), "Online Human Action Recognition Based on Incremental Learning of Weighted Covariance Descriptors", Information Sciences, 2018. [code](https://1drv.ms/u/s!AtC_QqTZiFdKbDIpUNbMdaIkF_8?e=fkWaq0)

16. **Pichao Wang**, Wanqing Li, Philip Ogunbona, Jun Wan and Sergio Escalera, "RGB-D-based Human Motion Recognition with Deep Learning: A Survey ", Computer Vision and Image Understanding, 2018.

17. **Pichao Wang**, Wanqing Li, Zhimin Gao, Chang Tang, and Philip Ogunbona, "Depth  Pooling Based Large-scale 3D Action Recognition with Deep Convolutional Neural Networks", IEEE Transactions on Multimedia, 2018. [paper](https://sites.google.com/site/pichaossites/resources/TMM.pdf?attredirects=0&d=1). [code](https://www.dropbox.com/s/zshzeyphaiuc3bw/Codes4Iso.zip?dl=0) 

18. **Pichao Wang**, Wanqing Li, Chuankun Li, and Yonghong Hou, "Action Recognition Based on Joint Trajectory Maps with Convolutional Neural Networks",
Knowledge-Based Systems,2018. [paper](https://www.sciencedirect.com/science/article/abs/pii/S0950705118302582). [code](https://sites.google.com/site/pichaossites/resources/JTM-rotate-ntu.zip?attredirects=0&d=1)

19. Yonghong Hou, Zhaoyang Li,  **Pichao Wang**@ and Wanqing Li, (@ Corresponding author), "Skeleton Optical Spectra Based Action Recognition Using Convolutional Neural Networks", IEEE Transactions on Circuits and Systems for Video Technology, 2016. [code](https://sites.google.com/site/pichaossites/resources/SOScodes.rar?attredirects=0&d=1)

20. Jing Zhang, Wanqing Li, Philip Ogunbona, **Pichao Wang** and Chang Tang, "RGB-D based Action Recognition Datasets: A Survey", Pattern Recognition, 2016.

21. **Pichao Wang**, Wanqing Li, Zhimin Gao, Jing Zhang,  Chang Tang, and Philip Ogunbona, "Action Recognition from Depth Maps Using Deep Convolutional Neural Networks", IEEE Transactions on Human Machine Systems, 2016. [code](https://sites.google.com/site/pichaossites/resources/THMS.zip?attredirects=0&d=1)

 

## Academic Activities

### Editorial Works:

1. Associate Editor, Computer Engineering(<<计算机工程>>, Chinese Journal), 2019-2024
2. Eiditorial Board of Young Scientists, Journal of Computer Science and Technology (JCST) (Tier 1, CCF B), 2022.7.1-2024.6.30
3. Area Chair, ICME, 2021,2022: Area Chair for Multimedia Analysis and Understanding (main area)

### Selected Invited Journal Reviewer: 
1. IEEE Transactions on Pattern Analysis and Machine Intelligence
2. IEEE Transactions on Image Processing
3. IEEE Transactions on Circuits and Systems for Video Technology
4. IEEE Transactions on Cybernetics
5. IEEE Transactions on Neural Networks and Learning Systems
6. IEEE Transactions on Industrial Information
7. IEEE Transactions on Audio, Speech and Language Processing
8. IEEE Transactions on Multimedia
9. ACM Transactions on Interactive Intelligent Systems
10. ACM Transactions on Multimedia Computing, Communications and Applications


### Conference Technical Program Committee Member:
1. ICCV2017,2019,2021,2023
2. CVPR2018,2019,2020,2021,2022,2023,2024
3. ICME2018,2019,2020,2021,2022
4. IJCAI2018,2019,2020,2021
5. ACCV2018,2020
6. WACV2019,2020,2021
7. AAAI2019,2020,2021,2022
8. ECCV2020,2022,2024
9. NIPS2020,2021,2022,2023
10. ICML2021,2022
11. ICLR2022,2023,2024

## Work Experience
1. 2018.9-2022.10: I was employed as a staff/senior algorithm engineer, and conducted research on various computer vision tasks.

2. 2017.10-2018.6: I was employed as a researcher at Motovis Inc, and I was in charge of Fixed-point quantization networks, pixel-level semantic labeling, intelligent headlight control.

3. 2013.07-2013.11: I was employed  as a Software Engineer at Beijing Hanze Technology Co., ltd  and I was in charge of the development of software about video enhancement, including FFMpeg video decoding, video enhancement algorithms, denoising algorithms,  and H.264 coding by CUDA.

4. 2011.05-2011.12: I was employed as a Software Engineer at Beijing Maystar Information Technology Co., ltd , and I was in charge of decrypting the Office documents based on GPU.

5. 2010.07-2011.06: I participated a National High-tech R&D Program (863 Program) project at Institute of Wideband Wireless Communication and 3D Imaging (IWWC&3DI): Multi-view video acquisition and demonstration system (2009AA011507). I was in charge of adaptive definition adjustment and format conversion in 3D video network and implemented the 3D video combination algorithm using paralleled methods based on CUDA.

## Datasets
1. [FT-HID Dataset](https://github.com/ENDLICHERE/FT-HID): The dataset contains more than 38K RGB samples, 38K depth samples, and about 20K skeleton sequences. 30 classes of daily actions are designed specically for multi-person interaction with a wearable device and three fixed cameras. FT-HID dataset has a comparable number of data, action classes, and scenes with other RGB-D action recognition datasets. It is more complex as the data is collected from 109 distinct subjects with large variations in gender, age, and physical condition. More importantly, to the best of our knowledge, it is the first large-scale RGB-D dataset that is collected from both TPV and FPV perspectives for action recognition. Please cite the following papers if you use the dataset: 
Zihui Guo, Yonghong Hou, Pichao Wang, Zhimin Gao, Mingliang Xu and Wanqing Li, "FT-HID: A Large Scale RGB-D Dataset for First and Third Person Human Interaction Analysis", Neural Computing and Applications, 2022. [paper](https://arxiv.org/pdf/2209.10155.pdf) [code](https://github.com/ENDLICHERE/FT-HID)

2. [UOW Online Action3D Dataset](https://uowmailedu-my.sharepoint.com/personal/wanqing_uow_edu_au/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fwanqing%5Fuow%5Fedu%5Fau%2FDocuments%2FResearchDatasets%2FUOWOnlineAction3D): this dataset consists of action sequences of skeleton videos, the 20 actions are from the original MSR Action3D Dataset. The action videos are recorded by Microsoft Kinect V.2 with average 20fms/s frame rate. There are 20 participants to perform these actions, every participant performs each action according to his/her personal habits.  For each participant, he/she first repeats each action 3--5 times, then performs 20 actions continuously in a random order. These continuous action sequences can be used for online action recognition testing. The repeated action sequences will be used for training. In order to make the dataset can be used for cross dataset test, the 20 participants perform the actions in 4 different environments.Please cite the following papers if you use the dataset:  
Chang Tang, Wanqing Li, Pichao Wang, Lizhe Wang, "Online Human Action Recognition Based on Incremental Learning of Weighted Covariance Descriptors", Information Sciences,vol.467,pp.219-237, 2018. [paper](https://www.researchgate.net/publication/326788254_Online_Human_Action_Recognition_Based_on_Incremental_Learning_of_Weighted_Covariance_Descriptors). [code](https://1drv.ms/u/s!AtC_QqTZiFdKbDIpUNbMdaIkF_8?e=fkWaq0)


<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=RBq5yEUg_GJFhndmotKK_BtMq2QqlElC7HIoAVp1bPs&cl=ffffff&w=a"></script>
